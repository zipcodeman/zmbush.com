% Search for all the places that say "PUT SOMETHING HERE".

\documentclass[11pt]{article}
\usepackage{amsmath,textcomp,amssymb,geometry,graphicx}

\def\Name{Zachary Bush}  % Your name
\def\Sec{Th 4-5; Starfield}  % Your discussion section
\def\Login{cs170-nx} % Your login
\def\HW{8}

\title{CS170--Spring 2012 --- Solutions to Homework \HW}
\author{\Name, section \Sec, \texttt{\Login}}
\markboth{CS170--Spring 2012 Homework \HW \Name, section \Sec}{CS170--Spring 2012
Homework \HW \Name, section \Sec, \texttt{\Login}}
\pagestyle{myheadings}

\begin{document}
\maketitle

\section*{1.}
You can rephrase this problem as a DAG. This DAG should be constructed such that
each element points only to locations that are at least k miles away from the
current location. With this DAG, we can then solve it with dynamic programming
as follows: 

We start with our $P(0) = 0$, and update as follows:
\begin{equation*}
P(i) = \max[P(i-1), p_i + P(i')]
\end{equation*}

Where $i'$ is the closest $i$ index such that it is at least $k$ miles behind
$i$

There are only two options for each position on the QVH. One, we could add a
yuckdonalds at that location, or we could not. If we don't add a yuckdonalds,
then our profit is guaranteed to be that of $P(i-1)$. If we do, however add a
yuckdonalds, the earliest position that is at least $k$ miles away from $i$
($P(i')$), plus the profit gained from the yuckdonalds at position $i$, must be
the maximum possible profit for that position. Therefore the maximum possible is
the maximum between adding or not adding a yuckdonalds at $i$.
\newpage
\section*{2.}
Our subproblem for this problem is the largest substring ending at $i$, and $j$
respectively ($S(i, j)$)

We start with:
\begin{eqnarray*}
S(0, 0) &=& 0\\
S(i, 0) &=& 0\\
S(0, j) &=& 0
\end{eqnarray*}

For all $1 \le i \le n$, and $1 \le j \le m$

Then to calculate another element, we can use the recursive function:
\begin{equation*}
S(i, j) = 
\begin{cases}
S(i-1, j-1) + 1, & \text{if } x_i = y_j\\
0,&\text{Otherwise}
\end{cases}
\end{equation*}

This is a very simple algorithm. Our initialization is clearly correct, if
either of our arguments are zero, then the length of the subset is clearly zero.
Then when we are looking at any $(i, j)$ pair, there are two options, either
$x_i = y_j$, or they aren't. If they aren't equal, then the value is clearly
$0$, and if they are, then the value is equal to $S(i-1, j-1) + 1$, since we are
simply adding a new character. 

There are $O(nm)$ subproblems like this to calculate, and each of them will take
constant time, therefore the entire algorithm will take $O(nm)$ time as
required.
\newpage
\section*{3.}
Collaborators: Hayg Astorian, Andr\'e Crabb, Mary Stufflebeam
\begin{enumerate}
\item[(a)]
A very simple example where the greedy algorithm would not work is the
following:
\begin{equation*}
2, 5, 100, 4
\end{equation*}

The optimal choice here would be to pick the $2$, because that would force your
opponent to reveal the $100$ to you. The greedy algorithm would chose $4$, which
is clearly not what we want. 
\item[(b)]
We construct a table with information about the possible moves. The $x$
coordinate represents the start of the remaining subsequence of cards, and $y$
coordinate represents the end of the remaining subsequence of cards. An entry in
this table represents the optimal move for a player when faced with this
remaining subsequence.

We initialize our net score function as such:
\begin{equation*}
N(a, a) = v_a
\end{equation*}

We can continue to calculate the values of this function as follows:
\begin{equation*}
N(a, b) = \max[v_a - N(a+1, b), v_b - N(a, b-1)]
\end{equation*}

From here, we can construct a table of optimal moves. Populate the table as
such:
\begin{equation*}
T(a, a) = N(a, a)
\end{equation*}

And for all other cases:
\begin{equation*}
T(a, b) =
\begin{cases}
\text{first}, & \text{if } v_a - N(a+1, b) > v_b - N(a, b-1)\\
\text{last}, & \text{Otherwise}
\end{cases}
\end{equation*}
\end{enumerate}

At any stage in the game, there are two possible moves for the player to make.
The player could take the first card ($a$) at which point his score would be
increased by $v_a$, and in all other moves, he will gain at most $-A(a+1, b)$.
The same argument can be made for taking the last card. 

There are $O(n^2)$ possible subproblems to calculate. Each of them take $O(1)$
time to compute. Therefor the total running time is $O(n^2)$ as required. 
\newpage
\section*{4.}
If we think of the optimal cost of a tree consisting of the words between index
$a$, and $b$, then we can define our function $T(a, b)$ as follows. If $a > b$,
then $T(a,b) = 0$. 

Additionally, in the range $[a,b]$, we can select a single element to be the
pivot. For each level of the tree, all items remaining in the tree must pay for
the comparison at that level, so we have the cost of all of those items at each
level as:
\begin{equation*}
Cl(a, b) = \sum_{x=a}^{b} p_x
\end{equation*}

In addition to this cost at the level, we must minimize the value across the
possible pivots.  So we must select a $y$ value such that the sub trees are
minimized. In other words:
\begin{equation*}
Cs(a, b) = \min_{y=a}^{b} [T(a, y-1) + T(y+1, b)]
\end{equation*}

Adding these values together we get the cost of the optimal tree:
\begin{equation*}
T(a, b) = Cl(a,b) + Cs(a,b)
\end{equation*}

Our final answer will be the value of $T(1, n)$.
\newpage
\section*{5.}
Collaborators: Andr\'e Crabb, Hayg Astorian

Recall the parallel implementation of Bor\r{u}vka's algorithm:
\begin{enumerate}
\item[(a)] Argue (succinctly but convincingly) that it takes parallel time
$O(\log^2 n)$.

Each iteration of Bor\r{u}vka's algorithm is done using pointer jumping, and
thus takes $O(\log n)$ time.  Each time you run bor\r{u}vka's algorithm, the
number of connected components is at least halved, so we only need to run the
algorithm $O(\log n)$ times. Therefore the total algorithm runs in: $O(\log^2
n)$.  \item[(b)] How much work is involved? Compared to the serial
implementation.

The amount of work done pointer jumping is $O(|E|\log |V|)$ because we are doing
$O(\log |V|)$ work on each of the $O(|E|)$ edges. We have to perform this
operation $O(\log |V|)$ times. Therefore the amount of work is $O(|E|\log^2
|V|)$

The amount of work done by the serial implementation is actually less, because
we have to do $O(|E|)$ work at most $O(\log |V|)$ times. Therefore our running
time is: $(|E|\log |V|)$.

\item[(c)] Suppose that we only count remote steps of the algorithm, steps in
which one processor must access data in another. How much is the parallel time
now? Do you think that it can it be improved?

Counting the amount of work done accesing remote data, we need only consider
steps where we are performing pointer jumping. The amount of work done pointer
jumping is still $O(\log n)$, and thus our total time we see is $O(\log^2 n)$.
\end{enumerate}
\newpage
\section*{6.}
In class we discussed Huffman's algorithm for encoding text and the Edit
Distance dynamic program for aligning sequences. The purpose of this assignment
is to help you find out about the important algorithms used, for these same two
tasks, in the world.
\begin{enumerate}
\item[(a)] Look up in the www the Lempel-Ziv algorithm (you can even watch an
animation!). Write a paragraph describing it and its use. What is lossless
compression?

Lempel-Ziv performs compression in such a way as to make the compression in one
pass through the file. We keep track of a table of character sequences an
associated values. We look up in the table to find the largest subsequence of
the current block in the table and insert that value. We then take the next
character, and append it to the current sequence and add it back to the table.
thus, commonly repeated sequences of letters can be encoded very compactly. 

Lossless compression is a method of shrinking the size of a data set in such a
way as to enable the data to be restored to it's original state without losing
any information. Any text compression algorithm needs to be lossless. Other
compression formats such as the image compression JPEG does not need to be
lossless.

\item[(b)] Repeat for BLAST. What are the main heuristics used?

BLAST is commonly used for gene sequencing, to determine how different two genes
are. It basically aligns the gene in such a way as to determine the minimum edit
distance from one gene to another. It is done through a search through the data,
and looking at all the words that are of length $k$, and then ranking the areas
of the text by similarity and repeating until the we arrive at a solution. 

The main heuristics used are Maximal Segment Pair measurement, which measure the
segments with large matching pairs, and MSP approximation, which does this
faster than a brute force search.
\end{enumerate}
\end{document}
