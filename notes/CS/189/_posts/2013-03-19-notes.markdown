---
layout: notes
title: CS 189 Lecture 3
topic: Loss Functions
---

## Support Vector Machines
- Soft margin case (Equation 47 in Burges paper)
  - Minimize $$\frac12 |w|^2 + (\sum \epsilon\_i - \sum \alpha\_i (y\_i(x\_i\*w
    + b) - 1 + \epsilon\_i) - \sum \mu\_i\epsilon\_i$$
  - We want: $$y\_i (x\_i\*w + b) - 1$$ to be greater than 0
    - If less than zero, we penalize by amount which is: $$y\_i(x\_i\*w + b) -
      1$$

## Logistic Regression
- Maximize 
    $$\sum y\_i ln \frac{1}{1 + e^{-\beta^Tx}} + (1-y\_i) ln(1 - 
      \frac{1}{1 + e^{-\beta^Tx}})$$

- Minimize:
    $$\sum y\_i ln(1 + e^{-\beta^Tx}) + (1-y\_i)(1 + e^{-\beta^Tx})$$

- We minimize (Logistic Loss!): $$\sum ln(1 + e^{-y\_if(x\_i)})$$

## Misclassification Loss
- $$sign(y\_i f(x\_i))$$
